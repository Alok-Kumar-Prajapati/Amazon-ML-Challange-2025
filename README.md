# Amazon-ML-Challange-2025
This is the solution proposed by my team "Not Converging Since 2023" for the Amazon ML Challange 2025.


# ML Challenge 2025: Smart Product Pricing

**Team:** Not Converging Since 2023   
**Members:** Alok Kumar Prajapati , Dhruv Singh Yadav , Dipanshu 

-----

## Project Overview

This repository contains the solution for the **ML Challenge 2025: Smart Product Pricing**. The goal of the challenge was to develop a machine learning model to analyze product details (text and images) and accurately predict product prices.

Our solution is a **two-stage ensemble** that first uses a multi-modal deep learning model to extract rich features and then stacks a LightGBM model to refine the final predictions. The core of our approach is a novel **Gated Fusion Block**, which dynamically weighs text, image, and numeric data before fusing them for a robust, context-aware representation.

## Methodology and Architecture

Our end-to-end pipeline is a hybrid approach designed to handle the multi-modal nature of the data (text, images, and numeric features).

### Key Preprocessing Steps

Before training, the raw data underwent significant preprocessing:

  * **Log Transformation:** To stabilize training on the skewed price data, we applied a log transformation to the target variable (`price`).
  * **Feature Engineering:** We parsed the unstructured `catalog_content` to extract implicit numeric data and engineer new features.
  * **Data Format:** Raw CSVs were processed and saved as Parquet files.

### Two-Stage Solution Pipeline

Our model pipeline consists of two main stages:

#### Stage 1: Deep Multi-Modal Feature Extraction (GodTierModel)

This stage generates rich, out-of-fold (OOF) predictions using a 5-fold cross-validated multi-modal model.

  * **Text Processing:** Text features are embedded using `microsoft/deberta-v3-base` with a max sequence length of 128.
  * **Image Processing:** Images (resized to 224x224) are processed using `openai/clip-vit-base-patch32`.
  * **Numeric Processing:** The 6 original numeric features are processed by a 2-layer MLP.
  * **Fusion:** All three embedding types (text, image, numeric) are fed into our **GatedFusion Block**. This block uses a gating mechanism and a Transformer Encoder to dynamically weigh and fuse the features.
  * **Objective:** The model was trained with a dual regression/classification objective.

#### Stage 2: LightGBM Stacking

The second stage refines the output from the neural network.

  * **Model:** A `LightGBM Regressor` with a Mean Absolute Error (MAE) objective.
  * **Features:** This model is trained on a combination of the **6 original numeric features** and the **OOF predictions** generated by the Stage 1 model.

-----

## File Descriptions

This repository is organized into three main scripts:

  * `preprocess.py`

      * This script handles all data preprocessing and feature engineering.
      * It parses the raw `train.csv` and `test.csv` files, extracts implicit numeric data from text , applies transformations, and saves the processed data as Parquet files for faster training.

  * `pipeline.py`

      * This script contains the complete training pipeline for the two-stage model.
      * It runs the **Stage 1 (GodTierModel)** training using 5-fold cross-validation to generate out-of-fold predictions.
      * It then trains the **Stage 2 (LightGBM)** stacking model on the OOF predictions and original numeric features.
      * All trained models (both DL folds and the final LGBM model) are saved to disk.

  * `predict_test.py`

      * This script manages the end-to-end inference pipeline.
      * It loads the saved models from all stages, processes the test data using the same preprocessing steps, and generates the final `test_out.csv` file in the required submission format.

-----

## Usage (Example Workflow)

1.  **Download Data:** Place the `train.csv` and `test.csv` files in the `dataset/` directory and download the images as specified in the problem statement.

2.  **Preprocess Data:**

    ```bash
    python preprocess.py
    ```

3.  **Train Models:**

    ```bash
    python pipeline.py
    ```

4.  **Generate Predictions:**

    ```bash
    python predict_test.py
    ```

    This will create the `test_out.csv` file for submission.

-----

## Performance

  * **44.47 SMAPE** (Symmetric Mean Absolute Percentage Error) on the 25K test samples on the leaderboard (live during the contest) ranked 138 at the end.
